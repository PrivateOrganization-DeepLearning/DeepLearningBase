{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "- - -\n",
    "\n",
    "## 1. Related work\n",
    " - - -\n",
    " \n",
    "Googlenet이 나온 이후  2015년 딥러닝 구조의 문제였던 degradation problem을 해결한 ResNet이 나왔다. 하지만 ResNet 이후 뚜렷한 모델의 변화가 없다가 이후 2017년 CVPR 컨퍼런스에 나온 네트워크 구조에 변화를 주는 모델이 DenseNet이다. \n",
    "\n",
    "## 2. Introduction\n",
    "- - - \n",
    "ResNet의 연구에서 Convolution network는 input layer, output layer에 가까울수록 더 정확하고 효율적이라는 것이 증명되었다. 따라서 Densenet에서도 이같은 원리를 이용하였다.\n",
    "\n",
    "일반적인 신경망에서와 달리 DenseNet에서는 L(L + 1) / 2개의 연결이 존재한다. 이에 대한 장점은 Degradation problem 해결, feature propagation 강화, 더 적은 parameter의 사용 등이 있다. \n",
    "\n",
    "## 3. Architecture\n",
    "\n",
    "- - -\n",
    "\n",
    "### Dense Connectivity\n",
    "![Alt text](DenseNet_image/image1.png)\n",
    "\n",
    "**ResNet**의 경우 이전의 layer 결과를 이후 layer에 더해주는 형태로 진행된다. 따라서 output에 대한 수식은 다음과 같이 정의된다.\n",
    "![Alt text](DenseNet_image/image2.png)\n",
    "\n",
    "**DenseNet**은 이전 layer의 결과를 concat하는 형태로 되어있다. 따라서 output에 대한 수식은 다음과 같다.\n",
    "![Alt text](DenseNet_image/image3.png)\n",
    "\n",
    "### Composite function\n",
    "위에 나온 비선형 함수 ***H()*** 는 batch normalization (BN), ReLU, 3 8 3 Convolution layer로 결합되어 있다. \n",
    "\n",
    "### Pooling layers\n",
    "위의 DenseNet 그림에서 Dense block 사이에 있는 Conv - Pooling layer는 **transition layer** 라고 칭하며 같은 Dense block에 있는 layer들은 모두 같은 feature map size를 갖게 된다. \n",
    "\n",
    "### Growth rate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
